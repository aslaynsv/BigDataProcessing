{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c5ece6-bb87-46d8-9e5c-26a472eba9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "base_path = \"/Volumes/workspace/default/bigdataproject/\"\n",
    "\n",
    "\n",
    "path_names = f\"{base_path}name.basics.tsv\"\n",
    "path_akas = f\"{base_path}title.akas.tsv\"\n",
    "path_basics = f\"{base_path}title.basics.tsv\"\n",
    "path_crew = f\"{base_path}title.crew.tsv\"\n",
    "path_episode = f\"{base_path}title.episode.tsv\"\n",
    "path_principals = f\"{base_path}title.principals.tsv\"\n",
    "path_ratings = f\"{base_path}title.ratings.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb328ed-8ebb-4f5c-b9ac-110df1ebcd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 2 : How many total people in data set ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e0f131-2589-4d0b-9872-503578662924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_names = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_names)\n",
    "\n",
    "# We count the rows\n",
    "total_people = df_names.count()\n",
    "\n",
    "print(f\"Total number of people in the Dataset : {total_people}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2441b24-85c7-406a-aaf7-b385efe61d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 3 & 7 :  What is the earliest year of birth? \n",
    "#                   What is the most recent date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9505c43f-55b6-4b25-a3a4-06094abf355a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col, current_date, year\n",
    "\n",
    "stats = df_names.select(\n",
    "    min(col(\"birthYear\")).alias(\"earliest\"),\n",
    "    max(col(\"birthYear\")).alias(\"most_recent\")\n",
    ").first()\n",
    "\n",
    "earliest_year = int(stats[\"earliest\"])\n",
    "most_recent_year = int(stats[\"most_recent\"])\n",
    "\n",
    "print(f\"Earliest birth year: {earliest_year}\")\n",
    "print(f\"Most recent birth year: {most_recent_year}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c7bd2f-d42a-43f8-8afc-be12f2eb3ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 4 : How many years ago was this person born?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f96ad3-db63-49c6-b27f-86fbc6d52357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "years_ago = 2025 - earliest_year\n",
    "print(f\"This person was born {years_ago} years ago.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f03acc45-d05b-4bf9-a1a4-b42f3fc922e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 5 : Using only the data in the data set, determine if this date of birth correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792a3fb5-d2ea-41a4-a04a-7f38005a7011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nThe person that was born the earliest :\")\n",
    "df_names.filter(col(\"birthYear\") == earliest_year).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d188f60e-ada3-48b9-babb-d1367a9791d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 6 : Explain the reasoning for the answer in a code comment or new markdown cell. \n",
    "\n",
    "Basically, we searched in the dataframe the row where the \"birthYear\" is equal to the \"earliest_year' that we found, and then we display the entire row.\n",
    "\n",
    "Obviously, it is an outlier, because nobody should be born that long time ago, in that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1849629c-0b4d-4a97-acb9-0456cda6921a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_basics = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_basics) \\\n",
    "    .withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\")) \\\n",
    "    .withColumn(\"startYear\", col(\"startYear\").cast(\"int\"))\n",
    "\n",
    "df_basics.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b824d4-284e-4203-b668-d09019246a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 8 : What percentage of the people do not have a listed date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf952f4-46e2-453c-9cb6-dc741c13cca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We check how many people there are in total in the dataset, and then how many missing values, and we can easily get the percentage of missing vlaues\n",
    "total_people = df_names.count()\n",
    "missing_birth_count = df_names.filter(col(\"birthYear\").isNull()).count()\n",
    "\n",
    "percentage_missing = (missing_birth_count / total_people) * 100\n",
    "\n",
    "print(f\"Percentage of people with no birth year: {percentage_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5be724cd-2ae7-4fc4-bb54-04fe61b3f539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 9 : What is the length of the longest \"short\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a49e38b-f556-404c-b181-d8093a102cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "# We filter for shorts, after 1900, and then find max runtime\n",
    "longest_short = df_basics.filter(\n",
    "    (col(\"titleType\") == \"short\") & \n",
    "    (col(\"startYear\") > 1900)\n",
    ").select(max(\"runtimeMinutes\")).first()[0]\n",
    "\n",
    "print(f\"Longest short film after 1900: {longest_short} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5024dda7-e11d-4e79-afa6-ce3c3b430fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 10 : What is the length of the shortest \"movie\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007654e0-e0db-443f-b912-2e22b10373e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "# We first filter for movies, after 1900, and then find min runtime\n",
    "shortest_movie = df_basics.filter(\n",
    "    (col(\"titleType\") == \"movie\") & \n",
    "    (col(\"startYear\") > 1900)\n",
    ").select(min(\"runtimeMinutes\")).first()[0]\n",
    "\n",
    "print(f\"Shortest movie after 1900: {shortest_movie} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b489599e-0fbc-460a-a3fb-578c688d048e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 11 : List of all of the genres represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d3babc-a5fe-4ba5-be00-fc6265bb3cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# We split the genres string into a list like for example [\"Action\", \"Comedy\"]\n",
    "# We then get distinct values and sort them\n",
    "unique_genres = df_basics.select(explode(split(col(\"genres\"), \",\")).alias(\"genre\")) \\\n",
    "    .select(\"genre\") \\\n",
    "    .distinct() \\\n",
    "    .sort(\"genre\")\n",
    "\n",
    "display(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d49cac-9872-47a0-8b8d-e0909df52219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 12 : What is the highest rated comedy \"movie\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380b19cd-aebb-4b88-8b85-f7ca4ea26397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# We load the Ratings data\n",
    "df_ratings = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_ratings) \\\n",
    "    .withColumn(\"averageRating\", col(\"averageRating\").cast(\"double\")) \\\n",
    "    .withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
    "\n",
    "# We join basics (Movies) with Ratings\n",
    "# We filter for 'movie' type and 'Comedy' genre\n",
    "# We then sort by rating and votes\n",
    "best_comedy = df_basics.join(df_ratings, \"tconst\") \\\n",
    "    .filter(col(\"titleType\") == \"movie\") \\\n",
    "    .filter(col(\"genres\").contains(\"Comedy\")) \\\n",
    "    .orderBy(col(\"averageRating\").desc(), col(\"numVotes\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "best_comedy_id = best_comedy['tconst']\n",
    "\n",
    "print(f\"Title: {best_comedy['primaryTitle']}\")\n",
    "print(f\"Rating: {best_comedy['averageRating']}\")\n",
    "print(f\"Votes: {best_comedy['numVotes']}\")\n",
    "print(f\"ID: {best_comedy_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc608784-5b5d-4509-b113-5e798324848e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 13 : Who was the director of the movie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128265fc-4ba0-4c4e-8300-1de22b0bdab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load Crew and Names data\n",
    "df_crew = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_crew)\n",
    "\n",
    "df_names = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_names)\n",
    "\n",
    "# Director's ID\n",
    "director_row = df_crew.filter(col(\"tconst\") == best_comedy_id).first()\n",
    "director_ids = director_row['directors']\n",
    "\n",
    "if director_ids:\n",
    "    # There can be multiple directors for a same movie, that is why we create a list\n",
    "    ids_list = director_ids.split(\",\")\n",
    "    \n",
    "    print(f\"Director ID(s): {ids_list}\")\n",
    "    \n",
    "    # we search these ids in the names table\n",
    "    directors = df_names.filter(col(\"nconst\").isin(ids_list)) \\\n",
    "                        .select(\"primaryName\") \\\n",
    "                        .collect()\n",
    "    \n",
    "    print(\"Director(s):\")\n",
    "    for row in directors:\n",
    "        print(f\"- {row['primaryName']}\")\n",
    "else:\n",
    "    print(\"No director listed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba20454-3268-49ef-83c2-c858a84326a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 14 : List, if any, the alternate titles for the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e570ce1-6240-428a-9df8-b9aa8b313324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load Alternate Titles data\n",
    "df_akas = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_akas)\n",
    "\n",
    "print(f\"Alternate titles for '{best_comedy['primaryTitle']}':\")\n",
    "\n",
    "df_akas.filter(col(\"titleId\") == best_comedy_id) \\\n",
    "    .select(\"title\", \"region\", \"language\") \\\n",
    "    .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IMDB_Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
