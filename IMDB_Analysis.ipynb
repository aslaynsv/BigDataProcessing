{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28739758-85d7-42a7-be09-61bbe0bee129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Project\n",
    "\n",
    "## Team : Arthur YOUNOUSSOV and Soufiane SAIDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fcb79d-736c-4b3b-b037-2a9f81998090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Files were manually uploaded to a Databricks Volume at /Volumes/workspace/default/bigdataproject/ because of their size which was too big.  \n",
    "\n",
    "Besides, with the free version, we couldn't access the url and download the data from there, we needed the premium version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6520dc-e6fc-4c13-93ba-4d1db055aa4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How to run the code ? Replace \"base_path\" with your own volume link on Databricks, and everything will run perfectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c5ece6-bb87-46d8-9e5c-26a472eba9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "base_path = \"/Volumes/workspace/default/bigdataproject/\"\n",
    "\n",
    "\n",
    "path_names = f\"{base_path}name.basics.tsv\"\n",
    "path_akas = f\"{base_path}title.akas.tsv\"\n",
    "path_basics = f\"{base_path}title.basics.tsv\"\n",
    "path_crew = f\"{base_path}title.crew.tsv\"\n",
    "path_episode = f\"{base_path}title.episode.tsv\"\n",
    "path_principals = f\"{base_path}title.principals.tsv\"\n",
    "path_ratings = f\"{base_path}title.ratings.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb328ed-8ebb-4f5c-b9ac-110df1ebcd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 2 : How many total people in data set ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e0f131-2589-4d0b-9872-503578662924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_names = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_names)\n",
    "\n",
    "# We count the rows\n",
    "total_people = df_names.count()\n",
    "\n",
    "print(f\"Total number of people in the Dataset : {total_people}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c1e2864-b4dd-4d3a-9de2-94c1ed0d100a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Output : Total number of people in the Dataset : 14953819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2441b24-85c7-406a-aaf7-b385efe61d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 3 & 7 :  What is the earliest year of birth? \n",
    "#                   What is the most recent date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9505c43f-55b6-4b25-a3a4-06094abf355a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col, current_date, year\n",
    "\n",
    "stats = df_names.select(\n",
    "    min(col(\"birthYear\")).alias(\"earliest\"),\n",
    "    max(col(\"birthYear\")).alias(\"most_recent\")\n",
    ").first()\n",
    "\n",
    "earliest_year = int(stats[\"earliest\"])\n",
    "most_recent_year = int(stats[\"most_recent\"])\n",
    "\n",
    "print(f\"Earliest birth year: {earliest_year}\")\n",
    "print(f\"Most recent birth year: {most_recent_year}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78cabc3c-9baa-41ca-8943-4af3fb2dcc0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output :\n",
    "- Earliest birth year: 4\n",
    "- Most recent birth year: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c7bd2f-d42a-43f8-8afc-be12f2eb3ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 4 : How many years ago was this person born?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f96ad3-db63-49c6-b27f-86fbc6d52357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "years_ago = 2025 - earliest_year\n",
    "print(f\"This person was born {years_ago} years ago.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17adedfa-212d-4581-88d7-519e0b56b222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : This person was born 2021 years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03acc45-d05b-4bf9-a1a4-b42f3fc922e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 5 : Using only the data in the data set, determine if this date of birth correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792a3fb5-d2ea-41a4-a04a-7f38005a7011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nThe person that was born the earliest :\")\n",
    "df_names.filter(col(\"birthYear\") == earliest_year).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d188f60e-ada3-48b9-babb-d1367a9791d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 6 : Explain the reasoning for the answer in a code comment or new markdown cell. \n",
    "\n",
    "Basically, we searched in the dataframe the row where the \"birthYear\" is equal to the \"earliest_year' that we found, and then we display the entire row.\n",
    "\n",
    "Obviously, it is an outlier, because nobody should be born that long time ago, in that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1849629c-0b4d-4a97-acb9-0456cda6921a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_basics = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_basics) \\\n",
    "    .withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\")) \\\n",
    "    .withColumn(\"startYear\", col(\"startYear\").cast(\"int\"))\n",
    "\n",
    "df_basics.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b824d4-284e-4203-b668-d09019246a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 8 : What percentage of the people do not have a listed date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf952f4-46e2-453c-9cb6-dc741c13cca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We check how many people there are in total in the dataset, and then how many missing values, and we can easily get the percentage of missing vlaues\n",
    "total_people = df_names.count()\n",
    "missing_birth_count = df_names.filter(col(\"birthYear\").isNull()).count()\n",
    "\n",
    "percentage_missing = (missing_birth_count / total_people) * 100\n",
    "\n",
    "print(f\"Percentage of people with no birth year: {percentage_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a07b12-a0f6-47ad-8563-190eec4bb718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : Percentage of people with no birth year: 95.58%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be724cd-2ae7-4fc4-bb54-04fe61b3f539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 9 : What is the length of the longest \"short\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a49e38b-f556-404c-b181-d8093a102cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "# We filter for shorts, after 1900, and then find max runtime\n",
    "longest_short = df_basics.filter(\n",
    "    (col(\"titleType\") == \"short\") & \n",
    "    (col(\"startYear\") > 1900)\n",
    ").select(max(\"runtimeMinutes\")).first()[0]\n",
    "\n",
    "print(f\"Longest short film after 1900: {longest_short} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "701eb89e-25f1-44b6-9e4f-5af8d2f7e564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : Longest short film after 1900: 1311 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5024dda7-e11d-4e79-afa6-ce3c3b430fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 10 : What is the length of the shortest \"movie\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007654e0-e0db-443f-b912-2e22b10373e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "# We first filter for movies, after 1900, and then find min runtime\n",
    "shortest_movie = df_basics.filter(\n",
    "    (col(\"titleType\") == \"movie\") & \n",
    "    (col(\"startYear\") > 1900)\n",
    ").select(min(\"runtimeMinutes\")).first()[0]\n",
    "\n",
    "print(f\"Shortest movie after 1900: {shortest_movie} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3a4713-164a-4182-af10-16cd1e1dcc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : Shortest movie after 1900: 1 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b489599e-0fbc-460a-a3fb-578c688d048e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 11 : List of all of the genres represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d3babc-a5fe-4ba5-be00-fc6265bb3cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# We split the genres string into a list like for example [\"Action\", \"Comedy\"]\n",
    "# We then get distinct values and sort them\n",
    "unique_genres = df_basics.select(explode(split(col(\"genres\"), \",\")).alias(\"genre\")) \\\n",
    "    .select(\"genre\") \\\n",
    "    .distinct() \\\n",
    "    .sort(\"genre\")\n",
    "\n",
    "display(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cf815ca-fae9-447e-b492-a7ea5fa628d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : \n",
    "- Action\n",
    "- Adult\n",
    "- Adventure\n",
    "- Animation\n",
    "- Biography\n",
    "- Comedy\n",
    "- Crime\n",
    "- Documentary\n",
    "- Drama\n",
    "- Family\n",
    "- Fantasy\n",
    "- Film-Noir\n",
    "- Game-Show\n",
    "- History\n",
    "- Horror\n",
    "- Music\n",
    "- Musical\n",
    "- Mystery\n",
    "- News\n",
    "- Reality-TV\n",
    "- Romance\n",
    "- Sci-Fi\n",
    "- Short\n",
    "- Sport\n",
    "- Talk-Show\n",
    "- Thriller\n",
    "- War\n",
    "- Western"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d49cac-9872-47a0-8b8d-e0909df52219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 12 : What is the highest rated comedy \"movie\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380b19cd-aebb-4b88-8b85-f7ca4ea26397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# We load the Ratings data\n",
    "df_ratings = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_ratings) \\\n",
    "    .withColumn(\"averageRating\", col(\"averageRating\").cast(\"double\")) \\\n",
    "    .withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
    "\n",
    "# We join basics (Movies) with Ratings\n",
    "# We filter for 'movie' type and 'Comedy' genre\n",
    "# We then sort by rating and votes\n",
    "best_comedy = df_basics.join(df_ratings, \"tconst\") \\\n",
    "    .filter(col(\"titleType\") == \"movie\") \\\n",
    "    .filter(col(\"genres\").contains(\"Comedy\")) \\\n",
    "    .orderBy(col(\"averageRating\").desc(), col(\"numVotes\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "best_comedy_id = best_comedy['tconst']\n",
    "\n",
    "print(f\"Title: {best_comedy['primaryTitle']}\")\n",
    "print(f\"Rating: {best_comedy['averageRating']}\")\n",
    "print(f\"Votes: {best_comedy['numVotes']}\")\n",
    "print(f\"ID: {best_comedy_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "335c8784-585d-4214-8fae-349dbea03db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : \n",
    "- Title: Space Melody\n",
    "- Rating: 10.0\n",
    "- Votes: 6\n",
    "- ID: tt32752452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc608784-5b5d-4509-b113-5e798324848e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 13 : Who was the director of the movie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128265fc-4ba0-4c4e-8300-1de22b0bdab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load Crew and Names data\n",
    "df_crew = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_crew)\n",
    "\n",
    "df_names = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_names)\n",
    "\n",
    "# Director's ID\n",
    "director_row = df_crew.filter(col(\"tconst\") == best_comedy_id).first()\n",
    "director_ids = director_row['directors']\n",
    "\n",
    "if director_ids:\n",
    "    # There can be multiple directors for a same movie, that is why we create a list\n",
    "    ids_list = director_ids.split(\",\")\n",
    "    \n",
    "    print(f\"Director ID(s): {ids_list}\")\n",
    "    \n",
    "    # we search these ids in the names table\n",
    "    directors = df_names.filter(col(\"nconst\").isin(ids_list)) \\\n",
    "                        .select(\"primaryName\") \\\n",
    "                        .collect()\n",
    "    \n",
    "    print(\"Director(s):\")\n",
    "    for row in directors:\n",
    "        print(f\"- {row['primaryName']}\")\n",
    "else:\n",
    "    print(\"No director listed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de4dbaf-3a71-4445-97ae-af3c84806f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : \n",
    "- Director ID(s): ['nm4492923']\n",
    "- Director(s):\n",
    "- Leonardo Thimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba20454-3268-49ef-83c2-c858a84326a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 14 : List, if any, the alternate titles for the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e570ce1-6240-428a-9df8-b9aa8b313324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load Alternate Titles data\n",
    "df_akas = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(path_akas)\n",
    "\n",
    "print(f\"Alternate titles for '{best_comedy['primaryTitle']}':\")\n",
    "\n",
    "df_akas.filter(col(\"titleId\") == best_comedy_id) \\\n",
    "    .select(\"title\", \"region\", \"language\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180ac314-f2ab-4e60-86ae-18c3a2da9555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : \n",
    "- Space Melody\n",
    "- H Melwdia Tou Diastimatos \n",
    "- Leonardo Thimo's Space Melody\n",
    "- Space Melody\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172ad030-7c6c-45d3-a77e-d463f7121c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Stream Processing: Wikipedia Edit Monitor\n",
    "\n",
    "For this part, we set up a real-time stream processing connected to the **Wikimedia Events Platform**. \n",
    "\n",
    "**Chosen Entities:**\n",
    "1.  **Comedy** (Genre)\n",
    "2.  **The Godfather** (Movie)\n",
    "3.  **Leonardo DiCaprio** (Actor)\n",
    "4.  **Christopher Nolan** (Director)\n",
    "5.  **Avatar** (Movie)\n",
    "\n",
    "\n",
    "We listen to the recentchange stream. If an edit occurs on a page matching one of our entities:\n",
    "1.  **Metrics:** We log the event (timestamp, user, title) to a standard metrics file (wiki_metrics.json).\n",
    "2.  **Alerts:** If the edit is \"suspicious\" or significant (change in length > 500 characters), we route this specific event to a separate alert file (wiki_alerts.json) to mimic a high-priority notification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9132648-157f-4f9e-8446-0e9729f9eb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sseclient-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81083871-c5ab-47eb-8242-3ea2b10ebcce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# CONFIGURATION\n",
    "watching_entities = [\"Comedy\", \"The Godfather\", \"Leonardo DiCaprio\", \"Christopher Nolan\", \"Avatar\"]\n",
    "base_path = \"/Volumes/workspace/default/bigdataproject/\"\n",
    "metrics_file = f\"{base_path}wiki_metrics.json\"\n",
    "alerts_file = f\"{base_path}wiki_alerts.json\"\n",
    "\n",
    "# Buffer lists to store data in memory before writing\n",
    "metrics_buffer = []\n",
    "alerts_buffer = []\n",
    "\n",
    "print(f\"Monitoring start for: {watching_entities}\")\n",
    "print(\"--- Listening for events (SIMULATION MODE) ---\")\n",
    "\n",
    "# We simulate data\n",
    "def simulate_wiki_stream():\n",
    "    users = [\"WikiBot\", \"JohnDoe\", \"Editor99\", \"Anonymous\"]\n",
    "    titles = [\"Random Page\", \"Comedy Club\", \"Avatar 2\", \"The Godfather Part III\", \"History of Jazz\"]\n",
    "    \n",
    "    # Generation of 20 simulated events\n",
    "    for i in range(20):\n",
    "        time.sleep(0.5) \n",
    "        \n",
    "        # Randomly decide if this event matches our watched entities\n",
    "        is_target = random.choice([True, False])\n",
    "        title = random.choice(watching_entities) if is_target else random.choice(titles)\n",
    "        \n",
    "        old_len = random.randint(1000, 5000)\n",
    "        new_len = old_len + random.randint(-600, 600)\n",
    "        \n",
    "        yield {\n",
    "            \"title\": title,\n",
    "            \"user\": random.choice(users),\n",
    "            \"timestamp\": time.time(),\n",
    "            \"length\": {\"old\": old_len, \"new\": new_len},\n",
    "            \"server_url\": \"https://en.wikipedia.org\"\n",
    "        }\n",
    "\n",
    "# Stream processing\n",
    "try:\n",
    "    for change in simulate_wiki_stream():\n",
    "        page_title = change.get('title', '')\n",
    "        \n",
    "        # FILTER LOGIC: Check if title contains one of our entities\n",
    "        if any(entity.lower() in page_title.lower() for entity in watching_entities):\n",
    "            \n",
    "            user = change.get('user')\n",
    "            timestamp = change.get('timestamp')\n",
    "            diff_len = change.get('length', {}).get('new', 0) - change.get('length', {}).get('old', 0)\n",
    "            \n",
    "            data_record = {\n",
    "                \"entity_match\": page_title,\n",
    "                \"user\": user,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"size_change\": diff_len\n",
    "            }\n",
    "            \n",
    "            metrics_buffer.append(data_record)\n",
    "            print(f\"[METRIC] Match found: {page_title}\")\n",
    "\n",
    "            # ALERT LOGIC: Trigger if size change > 500 chars\n",
    "            if abs(diff_len) > 500:\n",
    "                alert_record = data_record.copy()\n",
    "                alert_record[\"alert_type\"] = \"LARGE_EDIT_SIMULATED\"\n",
    "                alerts_buffer.append(alert_record)\n",
    "                print(f\"!!! [ALERT] Large change on {page_title} !!!\")\n",
    "\n",
    "    # Final write\n",
    "    print(\"\\nWriting files to Volume...\")\n",
    "    \n",
    "    # Writing metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        for record in metrics_buffer:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "    \n",
    "    # Writing alerts\n",
    "    with open(alerts_file, \"w\") as f:\n",
    "        for record in alerts_buffer:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "    print(f\"Success! Files created:\\n- {metrics_file}\\n- {alerts_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8192af-02b4-4b9e-acf2-9d0ee9c7f345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output : \n",
    "- [METRIC] Match found: Comedy\n",
    "- [METRIC] Match found: Comedy\n",
    "- [METRIC] Match found: Christopher Nolan\n",
    "- [METRIC] Match found: The Godfather\n",
    "- !!! [ALERT] Large change on The Godfather !!!\n",
    "- [METRIC] Match found: The Godfather\n",
    "- [METRIC] Match found: Comedy Club\n",
    "- [METRIC] Match found: Avatar\n",
    "- [METRIC] Match found: Avatar\n",
    "- [METRIC] Match found: The Godfather\n",
    "- !!! [ALERT] Large change on The Godfather !!!\n",
    "- [METRIC] Match found: Comedy\n",
    "- [METRIC] Match found: Avatar 2\n",
    "- [METRIC] Match found: Comedy Club\n",
    "- [METRIC] Match found: Comedy\n",
    "- !!! [ALERT] Large change on Comedy !!!\n",
    "- \n",
    "- Writing files to Volume...\n",
    "- Success! Files created:\n",
    "- /Volumes/workspace/default/bigdataproject/wiki_metrics.json\n",
    "- /Volumes/workspace/default/bigdataproject/wiki_alerts.json\n",
    "\n",
    "### As we can see, the code worked and it did create 2 files, one with metrics and the other one with alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7074e0f5-d37d-4a04-aea8-8bb4bcd7cf4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Why we performed this Stream Processing simulation\n",
    "\n",
    "The goal of this section was to demonstrate the ability to process **live, unbounded data** as opposed to static files. \n",
    "\n",
    "1. **Real-time Monitoring**: We chose 5 specific entities (Movies, Actors, Genres) to track how they are being discussed or updated on Wikipedia in real-time.\n",
    "2. **Filtering & Metrics**: Instead of saving every single Wikipedia edit (which would be millions of rows), our code acts as a filter. It only keeps data relevant to our project, saving these as \"Metrics\" for further analysis.\n",
    "3. **Alerting System**: We implemented a \"threshold-based alert.\" If an edit is unusually large (over 500 characters), it is automatically routed to a separate \"Alerts\" file. This mimics a professional system where an engineer would be notified of potential vandalism or major breaking news.\n",
    "\n",
    "# IMPORTANT\n",
    "\n",
    "4. **Technical Adaptation**: Due to cluster network restrictions preventing a live connection to stream.wikimedia.org (because of the free version), we developed a **Data Simulator**. This simulator generates JSON events identical to the official Wikimedia schema, proving that our processing logic, filtering, and storage system are fully functional."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IMDB_Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
